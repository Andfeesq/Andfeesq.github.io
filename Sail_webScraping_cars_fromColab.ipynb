{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andfeesq/BIG-DATA-SEMINARY.github.io/blob/main/Sail_webScraping_cars_fromColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d95f2ef3"
      },
      "source": [
        "# Scraping Used Car Web Data: Case Study tucarrro.com (Colab version) CODIGO ORIGINAL TOMADO DEL AUTOR ELIAS BUITRAGO BOLIVAR\n",
        "\n",
        "[Author: Elias Buitrago Bolivar](https://github.com/ebuitrago?tab=repositories)\n",
        "\n",
        "This jupyter notebook depicts a python based web scraping  algorithm to obtain data to train a price car prediction machine learning algorithm. Used cars web data are extracted from [Tu Carro](www.tucarro.com.co). The code presented here is functional and was tested by scraping real data. This code version is compatible with Colab.\n",
        "_Updated: Jun 20, 2024_\n"
      ],
      "id": "d95f2ef3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4c3640"
      },
      "source": [
        "## Install required libraries"
      ],
      "id": "bc4c3640"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79e2c30d",
        "outputId": "c4eb19a1-bc30-43fc-8047-5d6d8da9a934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.2)\n",
            "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (42.0.8)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.3.1)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.9.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.7.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.2.1)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.4.post2)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.1)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.2)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
            "Requirement already satisfied: automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: incremental>=22.10.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.6.2)\n",
            "Requirement already satisfied: requests-html in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.31.0)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.5.1)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from requests-html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.2.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2024.6.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.0.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (11.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.66.4)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.19)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.22.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.26.19)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml\n",
        "!pip install scrapy #MAS IMPORTANTE PARA SCRAPING\n",
        "!pip3 install requests-html\n",
        "!pip3 install selenium  #MAS IMPORTANTE PARA SCRAPING"
      ],
      "id": "79e2c30d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7h9wGJ10vay",
        "outputId": "81850a9b-caf0-4e23-cf6a-97b947bfe170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to ppa.\u001b[0m\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "You might want to run 'apt --fix-broken install' to correct these.\n",
            "The following packages have unmet dependencies:\n",
            " google-chrome-stable : Depends: libvulkan1 but it is not going to be installed\n",
            "\u001b[1;31mE: \u001b[0mUnmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\u001b[0m\n",
            "--2024-07-01 23:04:18--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 185.125.190.81, 185.125.190.82, 91.189.91.81, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|185.125.190.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb.5’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 23:04:18 (279 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb.5’ saved [3708/3708]\n",
            "\n",
            "(Reading database ... 122043 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) over (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2024-07-01 23:04:19--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 64.233.181.91, 64.233.181.93, 64.233.181.136, ...\n",
            "Connecting to dl.google.com (dl.google.com)|64.233.181.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 108773084 (104M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.5’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 103.73M   317MB/s    in 0.3s    \n",
            "\n",
            "2024-07-01 23:04:19 (317 MB/s) - ‘google-chrome-stable_current_amd64.deb.5’ saved [108773084/108773084]\n",
            "\n",
            "(Reading database ... 122043 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (126.0.6478.126-1) over (126.0.6478.126-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2024-07-01 23:04:33--  https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/120.0.6099.62/linux64/chromedriver-linux64.zip\n",
            "Resolving edgedl.me.gvt1.com (edgedl.me.gvt1.com)... 34.104.35.123, 2600:1900:4110:86f::\n",
            "Connecting to edgedl.me.gvt1.com (edgedl.me.gvt1.com)|34.104.35.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘/tmp/chromedriver-linux64.zip’ not modified on server. Omitting download.\n",
            "\n",
            "Archive:  /tmp/chromedriver-linux64.zip\n",
            "  inflating: /tmp/chromedriver-linux64/LICENSE.chromedriver  \n",
            "  inflating: /tmp/chromedriver-linux64/chromedriver  \n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.22.0)\n",
            "Requirement already satisfied: chromedriver_autoinstaller in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.26.19)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver_autoinstaller) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "%%shell\n",
        "# Install chromedriver\n",
        "# Credits: https://medium.com/@MinatoNamikaze02/running-selenium-on-google-colab-a118d10ca5f8\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/120.0.6099.62/linux64/chromedriver-linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver-linux64/chromedriver\n",
        "mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
        "\n",
        "pip install selenium chromedriver_autoinstaller"
      ],
      "id": "g7h9wGJ10vay"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "634627c4"
      },
      "source": [
        "### Web Scraping Used Car Sales Data\n",
        "This section explains the web scraping process implemented to obtain the data from the used car sales web site [Tu Carro](www.tucarro.com.co)."
      ],
      "id": "634627c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acbXwZHSLuSK",
        "outputId": "e098e6e4-2923-4764-b36a-04147e7e68d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: undetected_chromedriver in /usr/local/lib/python3.10/dist-packages (3.5.5)\n",
            "Requirement already satisfied: selenium>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from undetected_chromedriver) (4.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from undetected_chromedriver) (2.31.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from undetected_chromedriver) (10.4)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (1.26.19)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.9.0->undetected_chromedriver) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->undetected_chromedriver) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->undetected_chromedriver) (3.7)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected_chromedriver) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected_chromedriver) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected_chromedriver) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install undetected_chromedriver"
      ],
      "id": "acbXwZHSLuSK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cee4321"
      },
      "source": [
        "## Import required libraries\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "2cee4321"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p301W6XKx_3x",
        "outputId": "e7a6bba9-f164-4092-ab34-8aeecede2c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromedriver-autoinstaller in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver-autoinstaller) (24.1)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "credits:\n",
        "https://github.com/googlecolab/colabtools/issues/3347\n",
        "https://stackoverflow.com/questions/51046454/how-can-we-use-selenium-webdriver-in-colab-research-google-com\n",
        "Sept 19, 2023\n",
        "'''\n",
        "\n",
        "#\n",
        "!pip3 install chromedriver-autoinstaller"
      ],
      "id": "p301W6XKx_3x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6wYT0pimKyC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "import json"
      ],
      "id": "a6wYT0pimKyC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH4V_gqSoMY3"
      },
      "source": [
        "## Setup chrome and chrome driver\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "id": "yH4V_gqSoMY3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkwNiDUhoSIO",
        "outputId": "c130c55f-ee8d-4dd4-930c-a0865f8a6aa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/chromedriver_autoinstaller/126/chromedriver'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# # set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()"
      ],
      "id": "bkwNiDUhoSIO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtniE5ULxd13"
      },
      "source": [
        "\n",
        "## Section to declare functions\n",
        "\n",
        "---\n",
        "\n"
      ],
      "id": "BtniE5ULxd13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuMHDnfslfak"
      },
      "source": [
        "### Function scrapebyPages"
      ],
      "id": "RuMHDnfslfak"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b4086fc"
      },
      "outputs": [],
      "source": [
        "def scrapebyPages(brand,model,min, max):\n",
        "  #Range of pages from the total search to scrape in.\n",
        "  #It is recomended to cover a range of one hundred pages in each iteration of this section.\n",
        "  data = pd.DataFrame()\n",
        "  for i in range(min,max):\n",
        "\n",
        "      print(f'************************************')\n",
        "      print(f'WEB SCRAPING FROM SEARCH PAGE #{i}')\n",
        "      pag = i\n",
        "      url = f'https://vehiculos.tucarro.com.co/{brand}/{model}/_Desde_{49*i}_NoIndex_True'\n",
        "\n",
        "      driver = webdriver.Chrome(options=chrome_options)\n",
        "      driver.get(url)\n",
        "      driver.implicitly_wait(10)\n",
        "      html = driver.page_source\n",
        "      soup = bs(html,'lxml')\n",
        "\n",
        "      #Get href\n",
        "      links = gethref(soup)\n",
        "\n",
        "      p = []\n",
        "      #Scraping\n",
        "      for i in range(0,len(links)):\n",
        "          print('Scrapping', i, '/', len(links), '...')\n",
        "          p.append(scrapper(links[i]))\n",
        "          print(f'Este es el valor de p[i]: {p[i]}')\n",
        "\n",
        "      # append list to DataFrame\n",
        "      temp_df = pd.DataFrame(p)\n",
        "      data = pd.concat([data, temp_df], ignore_index=True)\n",
        "\n",
        "  #Close the web browser tab\n",
        "  driver.close()\n",
        "\n",
        "  # quit the driver\n",
        "  driver.quit()\n",
        "\n",
        "  return data"
      ],
      "id": "0b4086fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcITlNc7kZUU"
      },
      "source": [
        "### Function gethref"
      ],
      "id": "wcITlNc7kZUU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ae3aaa8"
      },
      "outputs": [],
      "source": [
        "#Function to get 'href' from each article item\n",
        "def gethref(soup):\n",
        "\n",
        "    links = []\n",
        "    for link in soup.findAll('a'):\n",
        "      url_car = link.get('href')\n",
        "      if 'MCO-' in url_car:\n",
        "        # print(url_car)          %Print each car url as a validity test\n",
        "        links.append(url_car)\n",
        "\n",
        "    print(\"Href obtained: \", len(links))\n",
        "\n",
        "    return links\n",
        "    # return"
      ],
      "id": "3ae3aaa8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGDlMylHklWl"
      },
      "source": [
        "### Function scrapper"
      ],
      "id": "OGDlMylHklWl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ed603ae"
      },
      "outputs": [],
      "source": [
        "#Function to call housing_features routine on each href\n",
        "def scrapper(url_car):\n",
        "\n",
        "    # set up the webdriver\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # Scrape\n",
        "    driver.get(url_car)\n",
        "    driver.implicitly_wait(10)\n",
        "    html=driver.page_source\n",
        "\n",
        "    #Obtaining the html from the web page after applying Selenium\n",
        "    soup = bs(html,'lxml')\n",
        "\n",
        "    #Create a list to store info obtained from one particular property\n",
        "    features = []\n",
        "\n",
        "    #Applying function to obtain variables defined from one particular property\n",
        "    features = extract_cars_features(soup)\n",
        "\n",
        "    #Close the web browser tab\n",
        "    driver.close()\n",
        "\n",
        "    # quit the driver\n",
        "    driver.quit()\n",
        "\n",
        "    return(features)"
      ],
      "id": "3ed603ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuAn5qaikpLH"
      },
      "source": [
        "### Function extract_cars_features"
      ],
      "id": "LuAn5qaikpLH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23acf2f0"
      },
      "outputs": [],
      "source": [
        "# Version 1.0\n",
        "def extract_cars_features(soup):\n",
        "\n",
        "  features_list = []\n",
        "\n",
        "  # car_name\n",
        "  try:\n",
        "    car_name = soup.find('h1',{'class': 'ui-pdp-title'}).text\n",
        "    features_list.append(car_name)\n",
        "    # print(f\"Car's name is: {car_name}\")\n",
        "  except:\n",
        "    car_name = ' '\n",
        "    features_list.append(car_name)\n",
        "\n",
        "  # price\n",
        "  try:\n",
        "    price=soup.find('div',{'class': 'ui-pdp-price__second-line'}).text\n",
        "    features_list.append(price)\n",
        "    # print(f\"Car's price is: {price}\")\n",
        "  except:\n",
        "    price = 0\n",
        "    features_list.append(price)\n",
        "\n",
        "  # year_car\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    year = year_kms_datePub[0]\n",
        "    features_list.append(year)\n",
        "  except:\n",
        "    year = 0\n",
        "    features_list.append(year)\n",
        "\n",
        "  # kms\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    kms = year_kms_datePub[2]\n",
        "    features_list.append(kms)\n",
        "  except:\n",
        "    kms = 0\n",
        "    features_list.append(kms)\n",
        "  # print(f\"Kms: {kms}\")\n",
        "\n",
        "# color and Fuel Type\n",
        "  try:\n",
        "    script = soup.find(\"script\", {'type': 'application/ld+json'})\n",
        "    if script:\n",
        "      # Obtain script content\n",
        "      script_text = json.loads(script.string)\n",
        "\n",
        "      # Extract json keys for color and fuel type\n",
        "      color = script_text.get('color', 'Color not found')\n",
        "      fuel = script_text.get('fuelType','Fuel type not found')\n",
        "\n",
        "      # Append results\n",
        "      features_list.extend([color, fuel])\n",
        "    else:\n",
        "      print(\"JavaScript script was not found on the page.\")\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(\"Error decoding JSON:\", str(e))\n",
        "      # Append default values in case of JSON decoding error\n",
        "      features_list.extend([0, 0])\n",
        "  except Exception as e:\n",
        "      print(\"An unexpected error occurred:\", str(e))\n",
        "      # Handle unexpected errors gracefully\n",
        "      features_list.extend([0, 0])\n",
        "\n",
        "\n",
        "  # print(features_list)\n",
        "\n",
        "\n",
        "  return features_list"
      ],
      "id": "23acf2f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVJILRDRovcE"
      },
      "source": [
        "## Start scraping\n",
        "\n",
        "---"
      ],
      "id": "aVJILRDRovcE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXxtjYNpq7_L"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By"
      ],
      "id": "gXxtjYNpq7_L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKU4dts4Hb2v",
        "outputId": "dcda86d5-4fc4-4f90-cc5a-3375662f75b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************\n",
            "WEB SCRAPING FROM SEARCH PAGE #1\n",
            "Href obtained:  0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        " The input parameters for the 'scrapebyPages' function are: Brand name, Car model\n",
        " name. Be careful to write the brand and model names exactly as they are in tucarro.com.\n",
        " The third input parameter is the initial results page (always initialize to 1)\n",
        " and the fourth input parameter is the final results page you want to download data from;\n",
        " this parameter depends on the amount of results pages your car returns\n",
        " for the brand and model you want to get data from. So, it is recommended to search\n",
        " the web portal first to find out how many pages of results you can get\n",
        " for the car you want to get data from.\n",
        "\"\"\"\n",
        "\n",
        "car_brand = 'renault'   # Brand car name. Ej: chevrolet, renault, kia.\n",
        "car_model = 'logan'        # Model car name. Ej: duster, onix, rio.\n",
        "data = scrapebyPages(car_brand,car_model,1,2)\n",
        "# scrapebyPages(1,2)"
      ],
      "id": "MKU4dts4Hb2v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "SalqwDzWlrqG",
        "outputId": "032da10d-fd52-461e-b39d-c0beb7587851"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 0 elements, new values have 6 elements",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-55021853140f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'car_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fueltype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6000\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \"\"\"\n\u001b[1;32m    729\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 6 elements"
          ]
        }
      ],
      "source": [
        "cols = ['car_model','price','year_model','kms','color','fueltype']\n",
        "data.columns = cols\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "id": "SalqwDzWlrqG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpUV4SB9lrsq"
      },
      "outputs": [],
      "source": [
        "saved_name=f'usedCarsCol_{car_model}_010724.csv'\n",
        "data.to_csv(saved_name, encoding='utf-8', index=False)"
      ],
      "id": "hpUV4SB9lrsq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOecwhsPrKQ6"
      },
      "source": [
        "### Testing code for scraping only one page\n",
        "This section provides a testing code for one page web scraping results"
      ],
      "id": "uOecwhsPrKQ6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_szzIxMF9dY"
      },
      "outputs": [],
      "source": [
        "#*****************************\n",
        "#Code for testing in one page\n",
        "#*****************************\n",
        "import json\n",
        "\n",
        "brand = 'kia'   # Brand car name. Ej: chevrolet, renault, kia.\n",
        "model = 'rio'   # Model car name. Ej: duster, onix, rio.\n",
        "\n",
        "# url = f'https://vehiculos.tucarro.com.co/{model}-{brand}'\n",
        "url = f'https://vehiculos.tucarro.com.co/{brand}/{model}/_Desde_{49*1}_NoIndex_True'\n",
        "print(url)\n",
        "\n",
        "#Function to call cars_features routine on each href\n",
        "def scrapper(url_car):\n",
        "\n",
        "    # set up the webdriver\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # Scrape\n",
        "    driver.get(url_car)\n",
        "    driver.implicitly_wait(10)\n",
        "    html=driver.page_source\n",
        "\n",
        "    #Obtaining the html from the web page after applying Selenium\n",
        "    soup = bs(html,'lxml')\n",
        "\n",
        "    #Create a list to store info obtained from one particular property\n",
        "    features = []\n",
        "\n",
        "    #Applying function to obtain variables defined from one particular property\n",
        "    features = extract_cars_features(soup)\n",
        "\n",
        "    #Close the web browser tab\n",
        "    driver.close()\n",
        "\n",
        "    # quit the driver\n",
        "    driver.quit()\n",
        "\n",
        "    return(features)\n",
        "\n",
        "\n",
        "def extract_cars_features(soup):\n",
        "\n",
        "  features_list = []\n",
        "\n",
        "  # car_name\n",
        "  try:\n",
        "    car_name = soup.find('h1',{'class': 'ui-pdp-title'}).text\n",
        "    features_list.append(car_name)\n",
        "    # print(f\"Car's name is: {car_name}\")\n",
        "  except:\n",
        "    car_name = ' '\n",
        "    features_list.append(car_name)\n",
        "\n",
        "  # price\n",
        "  try:\n",
        "    price=soup.find('div',{'class': 'ui-pdp-price__second-line'}).text\n",
        "    features_list.append(price)\n",
        "    # print(f\"Car's price is: {price}\")\n",
        "  except:\n",
        "    price = 0\n",
        "    features_list.append(price)\n",
        "\n",
        "  # year_car\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    year = year_kms_datePub[0]\n",
        "    features_list.append(year)\n",
        "  except:\n",
        "    year = 0\n",
        "    features_list.append(year)\n",
        "\n",
        "  # kms\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    kms = year_kms_datePub[2]\n",
        "    features_list.append(kms)\n",
        "  except:\n",
        "    kms = 0\n",
        "    features_list.append(kms)\n",
        "  # print(f\"Kms: {kms}\")\n",
        "\n",
        " # color and Fuel Type\n",
        "  try:\n",
        "    script = soup.find(\"script\", {'type': 'application/ld+json'})\n",
        "    if script:\n",
        "      # Obtain script content\n",
        "      script_text = json.loads(script.string)\n",
        "\n",
        "      # Extract json keys for color and fuel type\n",
        "      color = script_text.get('color', 'Color not found')\n",
        "      fuel = script_text.get('fuelType','Fuel type not found')\n",
        "\n",
        "      # Append results\n",
        "      features_list.extend([color, fuel])\n",
        "    else:\n",
        "      print(\"JavaScript script was not found on the page.\")\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(\"Error decoding JSON:\", str(e))\n",
        "      # Append default values in case of JSON decoding error\n",
        "      features_list.extend([0, 0])\n",
        "  except Exception as e:\n",
        "      print(\"An unexpected error occurred:\", str(e))\n",
        "      # Handle unexpected errors gracefully\n",
        "      features_list.extend([0, 0])\n",
        "\n",
        "  return features_list\n",
        "\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'lxml')\n",
        "\n",
        "#Get href\n",
        "links = []\n",
        "for link in soup.findAll('a'):\n",
        "  url_car = link.get('href')\n",
        "  if 'MCO-' in url_car:\n",
        "    links.append(url_car)\n",
        "print(\"Href obtained: \", len(links))\n",
        "\n",
        "p = []\n",
        "#Scraping\n",
        "for i in range(0,len(links)):\n",
        "  print('Scrapping', i, '/', len(links), '...')\n",
        "  p.append(scrapper(links[i]))\n",
        "  print(f'Este es el valor de p[i]: {p[i]}')\n",
        "\n",
        "temp_df = pd.DataFrame(p)\n",
        "# data = pd.concat([data, temp_df], ignore_index=True)\n",
        "\n",
        "#Close the web browser tab\n",
        "driver.close()\n",
        "\n",
        "# quit the driver\n",
        "driver.quit()\n",
        "\n",
        "\n",
        "temp_df.head()"
      ],
      "id": "L_szzIxMF9dY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c68c33"
      },
      "source": [
        "## Referencias\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/kiteco/kite-python-blog-post-code/blob/master/Web%20Scraping%20Tutorial/script.py\n",
        "\n",
        "https://medium.com/geekculture/scrappy-guide-to-web-scraping-with-python-475385364381\n",
        "\n",
        "https://stackoverflow.com/questions/47730671/python-3-using-requests-does-not-get-the-full-content-of-a-web-page"
      ],
      "id": "b4c68c33"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2cee4321",
        "yH4V_gqSoMY3",
        "wcITlNc7kZUU",
        "OGDlMylHklWl",
        "LuAn5qaikpLH",
        "uOecwhsPrKQ6"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}